

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>NN II – Compute Graph, Backprop and Training &#8212; Tools for Data Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '24-NN-II-Backprop';</script>
    <link rel="shortcut icon" href="_static/DS701-icon.jpg"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="NN III – Stochastic Gradient Descent, Batches and Convolutional Neural Networks" href="25-NN-III-CNNs.html" />
    <link rel="prev" title="Neural Networks I – Gradient Descent" href="23-NN-I-Gradient-Descent.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="landing-page.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/DS701-icon.jpg" class="logo__image only-light" alt="Tools for Data Science - Home"/>
    <script>document.write(`<img src="_static/DS701-icon.jpg" class="logo__image only-dark" alt="Tools for Data Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="landing-page.html">
                    Preface
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preliminaries</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-Intro-to-Python.html">Introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="02A-Git-Jupyter.html">Essential Tools: Git and Jupyter</a></li>
<li class="toctree-l1"><a class="reference internal" href="02B-Pandas.html">Essential Tools: Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-Linear-Algebra-Refresher.html">Linear Algebra Refresher</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-Probability-and-Statistics-Refresher.html">Probability and Statistics Refresher</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Clustering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05-Distances-Timeseries.html">Distances and Timeseries</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-Clustering-I-kmeans.html"><span class="math notranslate nohighlight">\(k\)</span>-means</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-Clustering-II-in-practice.html">Clustering In Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-Clustering-III-hierarchical.html">Hierarchical Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-Clustering-IV-GMM-EM.html">Gaussian Mixture Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Classification</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13-Learning-From-Data.html">Learning From Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-Classification-I-Decision-Trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-Classification-II-kNN.html"><span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbors and Model Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-Classification-III-NB-SVM.html">Naive Bayes and Support Vector Machines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dimensionality Reduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10-Low-Rank-and-SVD.html">Low Rank Approximation and the SVD</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-Dimensionality-Reduction-SVD-II.html">Dimensionality Reduction and PCA – SVD II</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Regression</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17-Regression-I-Linear.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-Regression-II-Logistic.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="19-Regression-III-More-Linear.html">Regularization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Networks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="23-NN-I-Gradient-Descent.html">Neural Networks I – Gradient Descent</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">NN II – Compute Graph, Backprop and Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="25-NN-III-CNNs.html">NN III – Stochastic Gradient Descent, Batches and Convolutional Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Selected Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="20-Recommender-Systems.html">Recommender Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="21-Networks-I.html">Introduction to Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="22-Networks-II-Centrality-Clustering.html">Network Centrality and Clustering</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/mcrovella/DS701-Tools-for-Data-Science" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/24-NN-II-Backprop.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>NN II – Compute Graph, Backprop and Training</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neuron-and-neural-networks-recap">Neuron and Neural Networks (Recap)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-neuron">Artificial Neuron</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relating-back-to-earlier-lectures">Relating Back to Earlier Lectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relating-back-to-another-earlier-lecture">Relating Back to <em>Another</em> Earlier Lecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-layer-perceptron-mlp-or-fully-connected-network-fcn">Multi-Layer Perceptron (MLP) or Fully Connected Network (FCN)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-graph">Computation Graph</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-value-class">Building the <code class="docutils literal notranslate"><span class="pre">Value</span></code> Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-addition">Implementing Addition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-more-operations">Implementing More Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#child-nodes">Child Nodes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#child-operations">Child Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-compute-graph">The Compute Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recap">Recap</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-gradients">Calculating Gradients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-gradient-calculation">Manual Gradient Calculation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#propagating-back">Propagating Back</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question">Question</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule">The Chain Rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#propagating-back-again">Propagating Back Again</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Recap</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-step-in-optimization">A Step in Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-single-neuron">A Single Neuron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coding-backpropagation">Coding Backpropagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accumulating-the-gradients">Accumulating the Gradients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#enhancements-to-value-class">Enhancements to <code class="docutils literal notranslate"><span class="pre">Value</span></code> Class</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-to-pytorch">Comparing to PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-modules">Neural Network Modules</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-and-evaluate-a-neuron">Initialize and Evaluate a Neuron</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-and-evaluate-a-layer-of-neurons">Initialize and Evaluate a Layer of Neurons</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-and-evaluate-an-mlp">Initialize and Evaluate an MLP</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop">Training Loop</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-and-train-the-equivalent-mlp-in-pytorch">Build and Train the Equivalent MLP in PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#to-dig-a-little-deeper">To Dig a Little Deeper</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="nn-ii-compute-graph-backprop-and-training">
<h1>NN II – Compute Graph, Backprop and Training<a class="headerlink" href="#nn-ii-compute-graph-backprop-and-training" title="Permalink to this heading">#</a></h1>
<p>In this lecture we’ll gradually build out a light weight neural network training framework reminiscent of PyTorch.</p>
<p>We build:</p>
<ul class="simple">
<li><p>A simple neural network engine we call <code class="docutils literal notranslate"><span class="pre">Value</span></code> class that wraps numbers and math operators and includes useful attributes and methods for implementing <em>forward pass</em> and <em>back propagation</em>. <em>(~63 lines of code)</em></p></li>
</ul>
<p>We’ll provide (and explain)</p>
<ul class="simple">
<li><p>A simple <em>compute graph</em> visualization function. <em>(34 lines of code)</em></p></li>
<li><p>A small set of helper functions that easily define a neuron, layer and multi-layer perceptron (MLP). <em>(84 lines of code)</em></p></li>
</ul>
<p>With that we can implement a neural network training loop, and see how similar it is to a PyTorch implementation.</p>
<hr class="docutils" />
<p>The code is based on Andrej Karpathy’s <a class="reference external" href="https://github.com/karpathy/micrograd">micrograd</a>.</p>
<section id="neuron-and-neural-networks-recap">
<h2>Neuron and Neural Networks (Recap)<a class="headerlink" href="#neuron-and-neural-networks-recap" title="Permalink to this heading">#</a></h2>
<p>Now let’s switch gears a bit to define an <em>artificial neuron</em>. For better or worse
it is named after and loosely modeled on a biological neuron.</p>
<!-- Image Credit "https://cs231n.github.io/neural-networks-1/"-->
<center>
<a class="reference internal image-reference" href="_images/neuron.png"><img alt="_images/neuron.png" src="_images/neuron.png" style="width: 75%;" /></a>
</center> 
<p>From <a class="reference external" href="https://cs231n.github.io/neural-networks-1/">cs231n</a></p>
<ul class="simple">
<li><p>The dendrites carry impulses from other neurons of different distances.</p></li>
<li><p>Once the collective firing rate of the impulses exceed a certain threshold, the neuron fires its own pulse through the axon to other neurons</p></li>
</ul>
<p>There are companies trying to mimic this impulse (i.e. spiking) based neuron in silicon – so called <em>neuromorphic computing</em>.</p>
<p>See for example <a class="reference external" href="https://en.wikipedia.org/wiki/Neuromorphic_engineering">Neuromorphic Computing</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/Spiking_neural_network">Spiking Neural Network</a></p>
<p>Some examples of companies and projects are Intel’s <a class="reference external" href="https://www.intel.com/content/www/us/en/research/neuromorphic-computing-loihi-2-technology-brief.html">Loihi</a> and startups such as GrAI Matter Labs <a class="reference external" href="https://www.graimatterlabs.ai/product">VIP processor</a>.</p>
<section id="artificial-neuron">
<h3>Artificial Neuron<a class="headerlink" href="#artificial-neuron" title="Permalink to this heading">#</a></h3>
<!-- Image Credit "https://cs231n.github.io/neural-networks-1/"-->
<center>
<a class="reference internal image-reference" href="_images/neuron_model.jpeg"><img alt="_images/neuron_model.jpeg" src="_images/neuron_model.jpeg" style="width: 75%;" /></a>
</center> 
<p>From <a class="reference external" href="https://cs231n.github.io/neural-networks-1/">cs231n</a></p>
<p>The more common artifical neuron</p>
<ul class="simple">
<li><p>collects one or more inputs,</p></li>
<li><p>each multiplied by a unique weight</p></li>
<li><p>sums the weighted inputs</p></li>
<li><p>adds a bias</p></li>
<li><p>then finally usually applies a nonlinear activation function</p></li>
</ul>
</section>
<section id="relating-back-to-earlier-lectures">
<h3>Relating Back to Earlier Lectures<a class="headerlink" href="#relating-back-to-earlier-lectures" title="Permalink to this heading">#</a></h3>
<p><strong>Question</strong></p>
<p>What does</p>
<div class="math notranslate nohighlight">
\[\sum_i w_i x_i + b\]</div>
<p>remind you of?</p>
<p><strong>Answer</strong></p>
<p>How about multiple regression in the Linear Regression lecture?</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1 u + \beta_2 v\]</div>
<p>We just renamed things: <span class="math notranslate nohighlight">\(\beta_0 \gets b\)</span>, <span class="math notranslate nohighlight">\(\beta_1 \gets w_0\)</span>, <span class="math notranslate nohighlight">\(\beta_2 \gets w_1\)</span>, <span class="math notranslate nohighlight">\(u \gets x_0\)</span> and <span class="math notranslate nohighlight">\(v \gets x_1\)</span> for <span class="math notranslate nohighlight">\(i \in [0,1]\)</span>.</p>
<blockquote>
<div><p>So multiple linear regression can be viewed as one linear neuron.</p>
</div></blockquote>
<p>Activation function is typically some nonlinear function that compresses the input in some way. Historically, it’s been the sigmoid and <span class="math notranslate nohighlight">\(\tanh()\)</span> functions. See for example <a class="reference external" href="https://en.wikipedia.org/wiki/Hyperbolic_functions#Tanh">Hyperbolic Functions</a>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sigmoid function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;sigmoid(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/097855c2cfb675ea430599a0c59d0d3bb563f7c5175c103b8d3a3591eff78cd7.png" src="_images/097855c2cfb675ea430599a0c59d0d3bb563f7c5175c103b8d3a3591eff78cd7.png" />
</div>
</div>
<p>The hyperbolic tangent, <span class="math notranslate nohighlight">\(\tanh\)</span>, is basically the sigmoid function shifted and scaled to a range of [-1,1].</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.2</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;tanh(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/f4ae910a2cba84f6a8db54f0074958b9283efd3c039cdaf1a27d847bb2ec0cca.png" src="_images/f4ae910a2cba84f6a8db54f0074958b9283efd3c039cdaf1a27d847bb2ec0cca.png" />
</div>
</div>
<p>A more common activation function these days and that is more efficient to implement is the <em>Rectified Linear Unit</em> or <em>ReLU</em>.</p>
<div class="math notranslate nohighlight">
\[ \textrm{ReLU}(x) = \mathrm{max}(0, x) \]</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.2</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ReLU(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/8518659a4721d1ff0db73361f9353fe2ec00d3845c3538c0e175a30d59ba3370.png" src="_images/8518659a4721d1ff0db73361f9353fe2ec00d3845c3538c0e175a30d59ba3370.png" />
</div>
</div>
<p>There are many other variations. See for example <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">PyTorch Non-linear Activations</a></p>
</section>
<section id="relating-back-to-another-earlier-lecture">
<h3>Relating Back to <em>Another</em> Earlier Lecture<a class="headerlink" href="#relating-back-to-another-earlier-lecture" title="Permalink to this heading">#</a></h3>
<p><strong>Question</strong></p>
<p>What does</p>
<div class="math notranslate nohighlight">
\[ \mathrm{sigmoid}(\sum_i w_i x_i + b) \hspace{10pt} \textrm{where} \hspace{10pt} \mathrm{sigmoid}(x) = \frac{1}{1 + e^{-x}}\]</div>
<p>remind you of?</p>
<p><strong>Answer</strong></p>
<p>How about the Logistic Regression model?</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
P(y=1\mid x) &amp; = \frac{e^{\alpha+\beta x}}{1+e^{\alpha+\beta x}}\\
             &amp; = \frac{e^{\alpha+\beta x}}{1+e^{\alpha+\beta x}} \left( \frac{e^{-(\alpha+\beta x)}}{e^{-(\alpha+\beta x)}} \right) \\
             &amp; = \frac{e^{(\alpha+\beta x)-(\alpha+\beta x)}}{e^{-(\alpha+\beta x)}+e^{(\alpha+\beta x)-(\alpha+\beta x)}} \\
             &amp; = \frac{e^0}{e^{-(\alpha+\beta x)} + e^0} \\
             &amp; = \frac{1}{1 + e^{-(\alpha+\beta x)}}
\end{split}
\end{split}\]</div>
<p>Which is the Sigmoid with <span class="math notranslate nohighlight">\(\alpha = 0\)</span> and <span class="math notranslate nohighlight">\(\beta = 1\)</span>.</p>
<p>In fact, just like in Logistic Regression, we use the sigmoid function on the last layer of a neural network that is
doing binary classification to output the probabilities.</p>
<blockquote>
<div><p>So Logistic Regression is similar to one neuron with a sigmoid activation function.</p>
</div></blockquote>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="o">-</span><span class="mi">12</span><span class="p">,</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">betas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">35</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span> 
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alphas</span><span class="p">)):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">alphas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">x</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\alpha=</span><span class="si">%d</span><span class="s2">,$    $\beta=</span><span class="si">%3.1f</span><span class="s2">$&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$p(x)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center left&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Logistic Functions&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/a055a164e73869a14305a0747b81a82b57e4dcc2146bf0c4f027a5ea45aac350.png" src="_images/a055a164e73869a14305a0747b81a82b57e4dcc2146bf0c4f027a5ea45aac350.png" />
</div>
</div>
</section>
<section id="multi-layer-perceptron-mlp-or-fully-connected-network-fcn">
<h3>Multi-Layer Perceptron (MLP) or Fully Connected Network (FCN)<a class="headerlink" href="#multi-layer-perceptron-mlp-or-fully-connected-network-fcn" title="Permalink to this heading">#</a></h3>
<center>
<a class="reference internal image-reference" href="_images/neural_net2.jpeg"><img alt="_images/neural_net2.jpeg" src="_images/neural_net2.jpeg" style="width: 75%;" /></a>
</center>
<p>From <a class="reference external" href="https://cs231n.github.io/convolutional-networks/">cs231n</a></p>
<p>Multiple artificial neurons can be acting on the same inputs, in what we call
a <em>layer</em>, and we can have more than one <em>layer</em> until we produce one or more
outputs.</p>
<p>The example above shows a network with <em>3 inputs</em>, two layers of neurons, each
with 4 neurons, followed by one layer that produces a single value output.</p>
<p>E.g. a binary classifier.</p>
</section>
</section>
<section id="computation-graph">
<h2>Computation Graph<a class="headerlink" href="#computation-graph" title="Permalink to this heading">#</a></h2>
<p>The way we are going to differentiate more complex functions is to first build a “computation graph” to apply our operations on. We’ll see that it breaks down the process into simple steps that easily scale to large networks.</p>
<p>It’s the concept employed by TensorFlow and PyTorch, and in fact we’ll follow PyTorch interface definition.</p>
<section id="building-the-value-class">
<h3>Building the <code class="docutils literal notranslate"><span class="pre">Value</span></code> Class<a class="headerlink" href="#building-the-value-class" title="Permalink to this heading">#</a></h3>
<p>To do that we will build a data wrapper as a <code class="docutils literal notranslate"><span class="pre">class</span></code> called <code class="docutils literal notranslate"><span class="pre">Value</span></code> and gradually build in on all the functionality we need to define a Multi-Layer Neural Network (a.k.a. Multi-Layer Perceptron) and train it.</p>
<p>First, the class has only a simple initialization method and a representation method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Value version 1</span>
<span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a string representation of the object for display&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Value(data=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">)&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">a</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=4.0)
</pre></div>
</div>
</div>
</div>
<p>If you are not familiar with <a class="reference external" href="https://docs.python.org/3/tutorial/classes.html">python classes</a>, there are a few things to note here.</p>
<ol class="arabic simple">
<li><p>The property <code class="docutils literal notranslate"><span class="pre">self</span></code> is just a pointer to the object itself.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method is called when you initialize a class object</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">__repr__</span></code> method is how you represent the class object</p></li>
</ol>
</section>
<section id="implementing-addition">
<h3>Implementing Addition<a class="headerlink" href="#implementing-addition" title="Permalink to this heading">#</a></h3>
<p>So the Value object doesn’t do much yet except for taking a value and printing it. We’d also like to do things like addition and other operations with them, but…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">a</span><span class="o">+</span><span class="n">b</span> 
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Uh oh!&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It worked!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Uh oh! unsupported operand type(s) for +: &#39;Value&#39; and &#39;Value&#39;
</pre></div>
</div>
</div>
</div>
<p>When python tries to add two objects <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>, internally it will call
<code class="docutils literal notranslate"><span class="pre">a.__add__(b)</span></code>. So we have to add the <code class="docutils literal notranslate"><span class="pre">__add__()</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Value version 2</span>
<span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a string representation of the object for display&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Value(data=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">a</span><span class="o">+</span><span class="n">b</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Uh oh!&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It worked!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>It worked!
</pre></div>
</div>
</div>
</div>
<p>Which, as mentioned is equivalent to calling the <code class="docutils literal notranslate"><span class="pre">__add__</span></code> method on <code class="docutils literal notranslate"><span class="pre">a</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="fm">__add__</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=1.0)
</pre></div>
</div>
</div>
</div>
</section>
<section id="implementing-more-operations">
<h3>Implementing More Operations<a class="headerlink" href="#implementing-more-operations" title="Permalink to this heading">#</a></h3>
<p>Similarly we can support multiplication and implement a ReLU function as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Value version 3</span>
<span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a string representation of the object for display&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Value(data=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="c1"># self + other</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="c1"># self * other</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">8.0</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span><span class="o">+</span><span class="n">c</span>
<span class="n">d</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=-4.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=0.0)
</pre></div>
</div>
</div>
</div>
<p>By the way, internally, python will call <code class="docutils literal notranslate"><span class="pre">__mul__</span></code> on <code class="docutils literal notranslate"><span class="pre">a</span></code>, then <code class="docutils literal notranslate"><span class="pre">__add__</span></code> on the temporary product object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="fm">__mul__</span><span class="p">(</span><span class="n">b</span><span class="p">))</span><span class="o">.</span><span class="fm">__add__</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=-4.0)
</pre></div>
</div>
</div>
</div>
</section>
<section id="child-nodes">
<h3>Child Nodes<a class="headerlink" href="#child-nodes" title="Permalink to this heading">#</a></h3>
<p>In order to calculate the gradients, we will need to capture the computation graphs. To do that, we’ll need to store pointers to the operands of each operation.</p>
<p>To start with, we’ll accept a tuple of child nodes in the initializer and store that as a set in the object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Value version 4</span>
<span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>
                        <span class="c1">#    vvvvvvvvvvvv</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">_children</span><span class="o">=</span><span class="p">()):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">_children</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a string representation of the object for display&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Value(data=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">))</span> <span class="c1"># store tuple of children</span>
        <span class="k">return</span> <span class="n">out</span>                        <span class="c1"># ^^^^^^^^^^^^^</span>
    
    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">))</span> <span class="c1"># store tuple of children</span>
        <span class="k">return</span> <span class="n">out</span>                        <span class="c1"># ^^^^^^^^^^^^^</span>
    
    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="p">,))</span>
        <span class="k">return</span> <span class="n">out</span>                         <span class="c1">#  ^^^^^^^</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">8.0</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span>
</pre></div>
</div>
</div>
</div>
<p>We can now see the children of the operands that produced the output value by printing the <code class="docutils literal notranslate"><span class="pre">_prev</span></code> value. The name <code class="docutils literal notranslate"><span class="pre">_prev</span></code> might not be intuitive yet, but it will make more sense when we view these operations as a graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="o">.</span><span class="n">_prev</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{Value(data=-3.0), Value(data=4.0)}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">e</span><span class="o">.</span><span class="n">_prev</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{Value(data=-12.0), Value(data=8.0)}
</pre></div>
</div>
</div>
</div>
</section>
<section id="child-operations">
<h3>Child Operations<a class="headerlink" href="#child-operations" title="Permalink to this heading">#</a></h3>
<p>Now we’ve recorded pointers to the child nodes. It would be helpful to also record the operator used.</p>
<p>We’ll also add labels for convenience.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Value version 5</span>
<span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>
                                    <span class="c1">#     vvvvvvv  vvvvvvvv</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">_children</span><span class="o">=</span><span class="p">(),</span> <span class="n">_op</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">_children</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op</span> <span class="o">=</span> <span class="n">_op</span> <span class="c1"># store the operation that created this node</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span> <span class="c1"># label for the node</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a string representation of the object for display&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Value(data=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>   
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="s1">&#39;+&#39;</span><span class="p">)</span> <span class="c1"># store tuple of children</span>
        <span class="k">return</span> <span class="n">out</span>                                      <span class="c1">#  ^^^</span>
    
    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="s1">&#39;*&#39;</span><span class="p">)</span> <span class="c1"># store tuple of children</span>
        <span class="k">return</span> <span class="n">out</span>                                      <span class="c1">#  ^^^</span>
    
    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>                                 <span class="c1">#  vvvvvv</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="s1">&#39;ReLU&#39;</span><span class="p">)</span>
        <span class="c1"># out = Value(0 if self.data &lt; 0 else self.data, (self,), &#39;ReLU&#39;)</span>

        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span> <span class="p">;</span> <span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span> <span class="p">;</span> <span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;e&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="o">.</span><span class="n">_prev</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">_op</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">label</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>({Value(data=-3.0), Value(data=4.0)}, &#39;*&#39;, &#39;d&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">e</span><span class="o">.</span><span class="n">_prev</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">_op</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">label</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>({Value(data=-12.0), Value(data=8.0)}, &#39;+&#39;, &#39;e&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-compute-graph">
<h3>The Compute Graph<a class="headerlink" href="#the-compute-graph" title="Permalink to this heading">#</a></h3>
<p>We now have enough information stored about the compute graph to visualize it.</p>
<p>These are two functions to walk the graph and build sets of all nodes and edges (<code class="docutils literal notranslate"><span class="pre">trace</span></code>) and then draw them as a
directed graph (<code class="docutils literal notranslate"><span class="pre">draw_dot</span></code>).</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># draw_dot version 1</span>
<span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Digraph</span>

<span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="n">root</span><span class="p">):</span>
  <span class="c1"># builds a set of all nodes and set of all edges in a graph</span>
  <span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="nb">set</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
      <span class="n">nodes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">_prev</span><span class="p">:</span>
        <span class="n">edges</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">child</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
        <span class="n">build</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
  <span class="n">build</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span>

<span class="k">def</span> <span class="nf">draw_dot</span><span class="p">(</span><span class="n">root</span><span class="p">):</span>
  <span class="n">dot</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;svg&#39;</span><span class="p">,</span> <span class="n">graph_attr</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;rankdir&#39;</span><span class="p">:</span> <span class="s1">&#39;LR&#39;</span><span class="p">})</span> <span class="c1"># LR = left to right</span>

  <span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span> <span class="o">=</span> <span class="n">trace</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
    <span class="n">uid</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="c1"># for any value in the graph, create a rectangular (&#39;record&#39;) node for it</span>
    <span class="n">dot</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="n">uid</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;{ </span><span class="si">%s</span><span class="s2"> | data </span><span class="si">%.4f</span><span class="s2"> }&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="s1">&#39;record&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">_op</span><span class="p">:</span>
      <span class="c1"># if this value is a result of some operation, create an op node for it</span>
      <span class="n">dot</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="n">uid</span> <span class="o">+</span> <span class="n">n</span><span class="o">.</span><span class="n">_op</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">_op</span><span class="p">)</span>
      <span class="c1"># and connect this no de to it</span>
      <span class="n">dot</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="n">uid</span> <span class="o">+</span> <span class="n">n</span><span class="o">.</span><span class="n">_op</span><span class="p">,</span> <span class="n">uid</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">n1</span><span class="p">,</span> <span class="n">n2</span> <span class="ow">in</span> <span class="n">edges</span><span class="p">:</span>
    <span class="c1"># connect n1 to the op node of n2</span>
    <span class="n">dot</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">n1</span><span class="p">)),</span> <span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">n2</span><span class="p">))</span> <span class="o">+</span> <span class="n">n2</span><span class="o">.</span><span class="n">_op</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">dot</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span> <span class="p">;</span> <span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span> <span class="p">;</span> <span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;e&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_dot</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/34fca75b252e1308b6df7ec59d49b880515158193ed935d09ca5b479f3aaa9de.svg" src="_images/34fca75b252e1308b6df7ec59d49b880515158193ed935d09ca5b479f3aaa9de.svg" /></div>
</div>
<p>Note that every value object becomes a node in the graph. The operators are also represented as a kind of fake node so they can be visualized too.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span> <span class="o">=</span> <span class="n">trace</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nodes: &quot;</span><span class="p">,</span> <span class="n">nodes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Edges: &quot;</span><span class="p">,</span> <span class="n">edges</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nodes:  {Value(data=4.0), Value(data=-3.0), Value(data=-4.0), Value(data=-12.0), Value(data=8.0)}
Edges:  {(Value(data=-3.0), Value(data=-12.0)), (Value(data=4.0), Value(data=-12.0)), (Value(data=-12.0), Value(data=-4.0)), (Value(data=8.0), Value(data=-4.0))}
</pre></div>
</div>
</div>
</div>
<p>Lets add one more operation, or stage in the compute graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span><span class="p">;</span> <span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span><span class="p">;</span> <span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;e&#39;</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>

<span class="n">L</span> <span class="o">=</span> <span class="n">e</span><span class="o">*</span><span class="n">f</span><span class="p">;</span> <span class="n">L</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;L&#39;</span>

<span class="n">draw_dot</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ab5495d2bd92c2ad30e4b5bc367b64c93f5bd47b9cf50e7f72605b9183d48a9b.svg" src="_images/ab5495d2bd92c2ad30e4b5bc367b64c93f5bd47b9cf50e7f72605b9183d48a9b.svg" /></div>
</div>
</section>
<section id="recap">
<h3>Recap<a class="headerlink" href="#recap" title="Permalink to this heading">#</a></h3>
<p>So far we’ve built a Value class and associated data structures to capture a computational graph and calculate the output based on the inputs and operations. We’ll call this the <strong>forward pass</strong>.</p>
<p>But now, we’re interested in calculating the gradients with respect to some of the parameters with respect to <span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>So next we’ll update our Value class to capture the partial derivative at each node relative to L.</p>
</section>
</section>
<section id="calculating-gradients">
<h2>Calculating Gradients<a class="headerlink" href="#calculating-gradients" title="Permalink to this heading">#</a></h2>
<p>Add a gradient member variable, <code class="docutils literal notranslate"><span class="pre">grad</span></code>, to our class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Value version 6</span>
<span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">_children</span><span class="o">=</span><span class="p">(),</span> <span class="n">_op</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># default to 0  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">_children</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op</span> <span class="o">=</span> <span class="n">_op</span> <span class="c1"># store the operation that created this node</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span> <span class="c1"># label for the node</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a string representation of the object for display&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Value(data=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>   
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="s1">&#39;+&#39;</span><span class="p">)</span> <span class="c1"># store tuple of children</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>   
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="s1">&#39;*&#39;</span><span class="p">)</span> <span class="c1"># store tuple of children</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>         
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="s1">&#39;ReLU&#39;</span><span class="p">)</span>
        <span class="c1"># out = Value(0 if self.data &lt; 0 else self.data, (self,), &#39;ReLU&#39;)</span>

        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<p>And update <code class="docutils literal notranslate"><span class="pre">draw_dot()</span></code> to show <code class="docutils literal notranslate"><span class="pre">grad</span></code> in the node info.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># draw_dot version 2</span>
<span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Digraph</span>

<span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="n">root</span><span class="p">):</span>
  <span class="c1"># builds a set of all nodes and set of all edges in a graph</span>
  <span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="nb">set</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
      <span class="n">nodes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">_prev</span><span class="p">:</span>
        <span class="n">edges</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">child</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
        <span class="n">build</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
  <span class="n">build</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span>

<span class="k">def</span> <span class="nf">draw_dot</span><span class="p">(</span><span class="n">root</span><span class="p">):</span>
  <span class="n">dot</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;svg&#39;</span><span class="p">,</span> <span class="n">graph_attr</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;rankdir&#39;</span><span class="p">:</span> <span class="s1">&#39;LR&#39;</span><span class="p">})</span> <span class="c1"># LR = left to right</span>

  <span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span> <span class="o">=</span> <span class="n">trace</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
    <span class="n">uid</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="c1"># for any value in the graph, create a rectangular (&#39;record&#39;) node for it</span>
    <span class="n">dot</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="n">uid</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;{ </span><span class="si">%s</span><span class="s2"> | data </span><span class="si">%.4f</span><span class="s2"> | grad </span><span class="si">%.4f</span><span class="s2"> }&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">grad</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="s1">&#39;record&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">_op</span><span class="p">:</span>
      <span class="c1"># if this value is a result of some operation, create an op node for it</span>
      <span class="n">dot</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="n">uid</span> <span class="o">+</span> <span class="n">n</span><span class="o">.</span><span class="n">_op</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">_op</span><span class="p">)</span>
      <span class="c1"># and connect this node to it</span>
      <span class="n">dot</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="n">uid</span> <span class="o">+</span> <span class="n">n</span><span class="o">.</span><span class="n">_op</span><span class="p">,</span> <span class="n">uid</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">n1</span><span class="p">,</span> <span class="n">n2</span> <span class="ow">in</span> <span class="n">edges</span><span class="p">:</span>
    <span class="c1"># connect n1 to the op node of n2</span>
    <span class="n">dot</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">n1</span><span class="p">)),</span> <span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">n2</span><span class="p">))</span> <span class="o">+</span> <span class="n">n2</span><span class="o">.</span><span class="n">_op</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">dot</span>        
</pre></div>
</div>
</div>
</div>
<p>And reinitialize and redraw…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span><span class="p">;</span> <span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span><span class="p">;</span> <span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;e&#39;</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>

<span class="n">L</span> <span class="o">=</span> <span class="n">e</span><span class="o">*</span><span class="n">f</span><span class="p">;</span> <span class="n">L</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;L&#39;</span>

<span class="n">draw_dot</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cd8e155cf53d06a6b6338b5901297abfa7fba0eda2ad4c128a13f30cf0cb6b11.svg" src="_images/cd8e155cf53d06a6b6338b5901297abfa7fba0eda2ad4c128a13f30cf0cb6b11.svg" /></div>
</div>
<section id="manual-gradient-calculation">
<h3>Manual Gradient Calculation<a class="headerlink" href="#manual-gradient-calculation" title="Permalink to this heading">#</a></h3>
<p>Before we start implementing backpropagation, it is helpful to manually calculate some gradients to better understand the procedure.</p>
<p>For the node <span class="math notranslate nohighlight">\(L\)</span>, we trivially calculate <span class="math notranslate nohighlight">\(\frac{dL}{dL} = 1\)</span>.</p>
<p>From limit ratio perspective,</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial L}{\partial L} = \lim_{h \rightarrow 0} \frac{ (L+h) - L }{h} = \frac{h}{h} = 1\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span>
</pre></div>
</div>
</div>
</div>
<p>If we go backwards a step in the graph, we see that <span class="math notranslate nohighlight">\(L=e*f\)</span>, so we calculate</p>
<div class="math notranslate nohighlight">
\[\frac{\partial{L}}{\partial{e}} = \frac{\partial}{\partial{e}} (e\times f) = f\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\frac{\partial{L}}{\partial{f}} = \frac{\partial}{\partial{f}} (e\times f) = e.\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">e</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p>To summarize, the partial derivative w.r.t. to one operand of a simple product is simply the other operand.</p>
</div></blockquote>
<p>And we can redraw the graph above again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_dot</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/87f7e18e45868daab1e1332180482879cf8b6ace02b14be0e06bcd3f0e22dd5b.svg" src="_images/87f7e18e45868daab1e1332180482879cf8b6ace02b14be0e06bcd3f0e22dd5b.svg" /></div>
</div>
<p>How do the parameters <span class="math notranslate nohighlight">\(e\)</span> and <span class="math notranslate nohighlight">\(f\)</span> influence <span class="math notranslate nohighlight">\(L\)</span>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Try uncommenting `e += h` or `f += h` and calling `wiggle()` then `wiggle(1.0)`</span>
<span class="c1"># to see the influence of e or f on L</span>
<span class="k">def</span> <span class="nf">wiggle</span><span class="p">(</span><span class="n">h</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>

    <span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span><span class="p">;</span> <span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span><span class="p">;</span> <span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;e&#39;</span>
    <span class="c1"># e += h</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">+=</span> <span class="n">h</span>

    <span class="n">L</span> <span class="o">=</span> <span class="n">e</span><span class="o">*</span><span class="n">f</span><span class="p">;</span> <span class="n">L</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;L&#39;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiggle</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=-8.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiggle</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=-12.0)
</pre></div>
</div>
</div>
</div>
</section>
<section id="propagating-back">
<h3>Propagating Back<a class="headerlink" href="#propagating-back" title="Permalink to this heading">#</a></h3>
<p>Now we want to calculate</p>
<div class="math notranslate nohighlight">
\[\frac{\partial{L}}{\partial{c}}\]</div>
<p>or put another way, we want to know how much <span class="math notranslate nohighlight">\(L\)</span> wiggles if we wiggle <span class="math notranslate nohighlight">\(c\)</span>, or how <span class="math notranslate nohighlight">\(c\)</span> influences <span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>Looking at the graph again we see that <span class="math notranslate nohighlight">\(c\)</span> influences <span class="math notranslate nohighlight">\(e\)</span> and <span class="math notranslate nohighlight">\(e\)</span> influences <span class="math notranslate nohighlight">\(L\)</span>, so we should be able see the ripple effect of <span class="math notranslate nohighlight">\(c\)</span> on <span class="math notranslate nohighlight">\(L\)</span>.</p>
<div class="math notranslate nohighlight">
\[ c \rightarrow e \rightarrow L \]</div>
<p>So <span class="math notranslate nohighlight">\(e = c + d\)</span>, and so we calculate</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial{e}}{\partial{c}} = \frac{\partial{}}{\partial{c}} (d + c) = 1\]</div>
</section>
<section id="question">
<h3>Question<a class="headerlink" href="#question" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[ c \rightarrow e \rightarrow L \]</div>
<p>So now we know <span class="math notranslate nohighlight">\(\partial{L}/\partial{e}\)</span> and we also know <span class="math notranslate nohighlight">\(\partial{e}/\partial{c}\)</span>,</p>
<p>How do we get <span class="math notranslate nohighlight">\(\partial{L}/\partial{c}\)</span>?</p>
</section>
<section id="the-chain-rule">
<h3>The Chain Rule<a class="headerlink" href="#the-chain-rule" title="Permalink to this heading">#</a></h3>
<p>To paraphrase from the Wikipedia page on <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule">Chain rule</a>, if a variable <span class="math notranslate nohighlight">\(L\)</span> depends on the variable <span class="math notranslate nohighlight">\(e\)</span>, which itself depends on the variable <span class="math notranslate nohighlight">\(c\)</span> (that is, <span class="math notranslate nohighlight">\(e\)</span> and <span class="math notranslate nohighlight">\(L\)</span> are dependent variables), then <span class="math notranslate nohighlight">\(L\)</span> depends on <span class="math notranslate nohighlight">\(c\)</span> as well, via the intermediate variable <span class="math notranslate nohighlight">\(e\)</span>. In this case, the chain rule is expressed as</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial c} = \frac{\partial L}{\partial e} \cdot \frac{\partial e}{\partial c},\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[ \left.\frac{\partial L}{\partial c}\right|_{c} = \left.\frac{\partial L}{\partial e}\right|_{e(c)}\cdot \left. \frac{\partial e}{\partial c}\right|_{c} ,\]</div>
<p>for indicating at which points the derivatives have to be evaluated.</p>
<p>Now since we’ve established that</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial{e}}{\partial{c}} = 1\]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[\frac{dL}{dc} = \frac{dL}{de} \cdot 1.\]</div>
<p>So in the case of an operand in an addition operation, we just copy the gradient of the parent node.</p>
<p>Or put another way,</p>
<blockquote>
<div><p>in the addition operator, we just route the parent gradient to the child.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">grad</span>
<span class="n">c</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_dot</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9c563fdd23a833bb4034139970a8eefb2d363931dbe2c4c39d0dd7a682bfe23f.svg" src="_images/9c563fdd23a833bb4034139970a8eefb2d363931dbe2c4c39d0dd7a682bfe23f.svg" /></div>
</div>
<p>How does <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(d\)</span> influence <span class="math notranslate nohighlight">\(L\)</span>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Try uncommenting `c += h` or `d += h` and calling `wiggle()` then `wiggle(1.0)`</span>
<span class="c1"># to see the influence of c or d on L</span>
<span class="k">def</span> <span class="nf">wiggle</span><span class="p">(</span><span class="n">h</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>
    <span class="c1"># c += h</span>

    <span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span><span class="p">;</span> <span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span>
    <span class="c1"># d += h</span>

    <span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span><span class="p">;</span> <span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;e&#39;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>

    <span class="n">L</span> <span class="o">=</span> <span class="n">e</span><span class="o">*</span><span class="n">f</span><span class="p">;</span> <span class="n">L</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;L&#39;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiggle</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=-8.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiggle</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=-8.0)
</pre></div>
</div>
</div>
</div>
</section>
<section id="propagating-back-again">
<h3>Propagating Back Again<a class="headerlink" href="#propagating-back-again" title="Permalink to this heading">#</a></h3>
<p>Now we want to calculate</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial{L}}{\partial{b}} \hspace{10pt} \textrm{and} \hspace{10pt} \frac{\partial{L}}{\partial{a}}\]</div>
<p>But we have</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial{L}}{\partial{d}}\]</div>
<p>and we know that</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial{d}}{\partial{b}} = \frac{\partial{}}{\partial{b}}(a\cdot b) = a\]</div>
<p>so again from the chain rule</p>
<div class="math notranslate nohighlight">
\[\frac{\partial{L}}{\partial{b}} 
  = \frac{\partial{L}}{\partial{d}} \cdot \frac{\partial{d}}{\partial{b}}
  = \frac{\partial{L}}{\partial{d}} \cdot a\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">d</span><span class="o">.</span><span class="n">grad</span>
<span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">d</span><span class="o">.</span><span class="n">grad</span>
<span class="n">draw_dot</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dbd9d936392024a1539df196cdf55244a2727a839a97579a55afcc747637e287.svg" src="_images/dbd9d936392024a1539df196cdf55244a2727a839a97579a55afcc747637e287.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Try uncommenting `a += h` or `b += h` and calling `wiggle()` then `wiggle(1.0)`</span>
<span class="c1"># to see the influence of a or b on L</span>
<span class="k">def</span> <span class="nf">wiggle</span><span class="p">(</span><span class="n">h</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
    <span class="c1"># a += h</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">+=</span> <span class="n">h</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>

    <span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span><span class="p">;</span> <span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span>

    <span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span><span class="p">;</span> <span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;e&#39;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>

    <span class="n">L</span> <span class="o">=</span> <span class="n">e</span><span class="o">*</span><span class="n">f</span><span class="p">;</span> <span class="n">L</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;L&#39;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiggle</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=-8.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiggle</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=0.0)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h3>Recap<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>As you saw, we recursively went backwards through the computation graph and applied the local gradients to the gradients calculated so far to get the partial gradients. Put another we propagated this calculations backwards through the graph.</p>
<p>Of course, in practice, we will only need the gradients on the parameters, not the inputs, so we won’t bother calculating them on inputs.</p>
<p><em>That is the essence of Back Propagation.</em></p>
</section>
</section>
<section id="a-step-in-optimization">
<h2>A Step in Optimization<a class="headerlink" href="#a-step-in-optimization" title="Permalink to this heading">#</a></h2>
<p>Let’s take a look at the graph again. Assume we want the value of L to <em>decrease</em>. We are free to change the values of the leaf nodes – all the other nodes are derived from children and leaf nodes.</p>
<p>The leaf nodes are <span class="math notranslate nohighlight">\(a, b, c\)</span> and <span class="math notranslate nohighlight">\(f\)</span>.</p>
<blockquote>
<div><p>Again, in practice we would only update the parameter leaf nodes, not the input leaf node, but we’ll ignore that distinction temporarily for this exmaple.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_dot</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dbd9d936392024a1539df196cdf55244a2727a839a97579a55afcc747637e287.svg" src="_images/dbd9d936392024a1539df196cdf55244a2727a839a97579a55afcc747637e287.svg" /></div>
</div>
<p>Let’s check the current value of L.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># remind ourselves what L is</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-8.0
</pre></div>
</div>
</div>
</div>
<p>As we showed before, we want to nudge each of those leaf nodes by the negative
of the gradient, multiplied by a step size, <span class="math notranslate nohighlight">\(\eta\)</span>.</p>
<div class="math notranslate nohighlight">
\[ w_{n+1} = w_n - \eta * \frac{\partial{L}}{\partial{w_n}} \]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the iteration number.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># nudge all the leaf nodes along the negative direction of the gradient</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.01</span>    <span class="c1"># also called eta above</span>

<span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="n">b</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
<span class="n">c</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">c</span><span class="o">.</span><span class="n">grad</span>
<span class="n">f</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">f</span><span class="o">.</span><span class="n">grad</span>

<span class="c1"># recompute the forward pass</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">b</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">e</span><span class="o">*</span><span class="n">f</span>

<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-9.230591999999998
</pre></div>
</div>
</div>
</div>
</section>
<section id="a-single-neuron">
<h2>A Single Neuron<a class="headerlink" href="#a-single-neuron" title="Permalink to this heading">#</a></h2>
<p>Let’s now programmatically define a single neuron with</p>
<ul class="simple">
<li><p>two inputs</p></li>
<li><p>two weights (1 for each input)</p></li>
<li><p>a bias</p></li>
<li><p>the ReLU activation function</p></li>
</ul>
<p>Recall the neuron figure above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># inputs x0, x1</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>

<span class="c1"># weights w1, w2</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;w1&#39;</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;w2&#39;</span><span class="p">)</span>

<span class="c1"># bias of the neuron</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">6.8813735870195432</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>

<span class="n">x1w1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">*</span><span class="n">w1</span><span class="p">;</span> <span class="n">x1w1</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;x1*w1&#39;</span>
<span class="n">x2w2</span> <span class="o">=</span> <span class="n">x2</span><span class="o">*</span><span class="n">w2</span><span class="p">;</span> <span class="n">x2w2</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;x2*w2&#39;</span>

<span class="n">x1w1x2w2</span> <span class="o">=</span> <span class="n">x1w1</span> <span class="o">+</span> <span class="n">x2w2</span><span class="p">;</span> <span class="n">x1w1x2w2</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;x1w1 + x2w2&#39;</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">x1w1x2w2</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span> <span class="n">n</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;n&#39;</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">relu</span><span class="p">();</span> <span class="n">o</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_dot</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9b693ee9b0b29bf8e9c4020113b38ec263de90021e7ecad14ad41277c5f59e31.svg" src="_images/9b693ee9b0b29bf8e9c4020113b38ec263de90021e7ecad14ad41277c5f59e31.svg" /></div>
</div>
<p>The only new operation we’ve added is the ReLU, so let’s take a quick look at how we
differentiate the ReLU.</p>
<p>Like before, for the output node, o:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial o}{\partial o} = 1 \]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">o</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span>
</pre></div>
</div>
</div>
</div>
<!-- Credit: _Understanding Deep Learning_, Figure 7.6 -->
<p>ReLU is technically not differentiable at 0, but practically we implement the
derivative as 0 when <span class="math notranslate nohighlight">\( \le 0\)</span> and 1 when <span class="math notranslate nohighlight">\( 1 &gt; 0 \)</span></p>
<center>
<a class="reference internal image-reference" href="_images/Train2ReLUDeriv.svg"><img alt="_images/Train2ReLUDeriv.svg" src="_images/Train2ReLUDeriv.svg" width="50%" /></a>
</center><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">o</span><span class="o">.</span><span class="n">grad</span>  <span class="c1"># = 0 when o.data &lt;= 0; = o.grad when o.data &gt; 0</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="coding-backpropagation">
<h2>Coding Backpropagation<a class="headerlink" href="#coding-backpropagation" title="Permalink to this heading">#</a></h2>
<p>Now we’ll update our <code class="docutils literal notranslate"><span class="pre">Value</span></code> class once more to support the backward pass.</p>
<p>There’s a</p>
<ul class="simple">
<li><p>private <code class="docutils literal notranslate"><span class="pre">_backward()</span></code> function <em>in each operator</em> that implements the local
step of the chain rule, and</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">backward()</span></code> function in the class that topologically sorts the graph and calls the operator <code class="docutils literal notranslate"><span class="pre">_backward()</span></code> function starting at the end of the graph and going <em>backward</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># version 9</span>
<span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">_children</span><span class="o">=</span><span class="p">(),</span> <span class="n">_op</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># default to 0, no impact on the output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="kc">None</span>  <span class="c1"># by default backward doesn&#39;t do anything</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">_children</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op</span> <span class="o">=</span> <span class="n">_op</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a string representation of the object for display&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Value(data=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">, grad=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="s1">&#39;+&#39;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="s1">&#39;*&#39;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="s1">&#39;ReLU&#39;</span><span class="p">)</span>
        <span class="c1"># out = Value(0 if self.data &lt; 0 else self.data, (self,), &#39;ReLU&#39;)</span>

        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="c1"># topological order all of the children in the graph</span>
        <span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">build_topo</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
                <span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">_prev</span><span class="p">:</span>
                    <span class="n">build_topo</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
                <span class="n">topo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">build_topo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># go one variable at a time and apply the chain rule to get its gradient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
            <span class="n">v</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We redefined the class so we have to reinitialize the objects and run the operations again.</p>
<p>This constitutes the <em>forward pass</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># inputs x0, x1</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>

<span class="c1"># weights w1, w2</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;w1&#39;</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;w2&#39;</span><span class="p">)</span>

<span class="c1"># bias of the neuron</span>
<span class="c1">#b = Value(6.7, label=&#39;b&#39;)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">6.8813735870195432</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>

<span class="n">x1w1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">*</span><span class="n">w1</span><span class="p">;</span> <span class="n">x1w1</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;x1*w1&#39;</span>
<span class="n">x2w2</span> <span class="o">=</span> <span class="n">x2</span><span class="o">*</span><span class="n">w2</span><span class="p">;</span> <span class="n">x2w2</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;x2*w2&#39;</span>

<span class="n">x1w1x2w2</span> <span class="o">=</span> <span class="n">x1w1</span> <span class="o">+</span> <span class="n">x2w2</span><span class="p">;</span> <span class="n">x1w1x2w2</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;x1w1 + x2w2&#39;</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">x1w1x2w2</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span> <span class="n">n</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;n&#39;</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">relu</span><span class="p">();</span> <span class="n">o</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span>
</pre></div>
</div>
</div>
</div>
<p>So we’ve filled the data values for all the nodes, but haven’t calculated the gradients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_dot</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b4b99f145d8f4adfab41741fcf69a708d070dacae7da83a32fde69952af581f7.svg" src="_images/b4b99f145d8f4adfab41741fcf69a708d070dacae7da83a32fde69952af581f7.svg" /></div>
</div>
<p>Now, all we have to do is call the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> method of the last node…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">o</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>And voila! We have all the gradients!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_dot</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/768a3a116a6bd27a41ef791e84dfc761a4ce34107e0fa8b6723dd06afe5fc58c.svg" src="_images/768a3a116a6bd27a41ef791e84dfc761a4ce34107e0fa8b6723dd06afe5fc58c.svg" /></div>
</div>
<section id="accumulating-the-gradients">
<h3>Accumulating the Gradients<a class="headerlink" href="#accumulating-the-gradients" title="Permalink to this heading">#</a></h3>
<p>The observant viewer will notice that we are accumulating the gradients.</p>
<p>That is to handle cases like where a <code class="docutils literal notranslate"><span class="pre">Value</span></code> object is on both sides of the operand like</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">a</span> <span class="p">;</span> <span class="n">b</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span>
<span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">a</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=3.0, grad=2.0)
</pre></div>
</div>
</div>
</div>
<p>If we didn’t have the accumulation, then <code class="docutils literal notranslate"><span class="pre">a.grad</span> <span class="pre">=</span> <span class="pre">1</span></code> instead.</p>
<p>Or the other case where a node goes to different operations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>  <span class="p">;</span> <span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>   <span class="p">;</span> <span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;e&#39;</span>
  
<span class="n">draw_dot</span><span class="p">(</span> <span class="n">f</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05e7ec0d603dcc732997517b7cd9a55d8ea90a34d28f81bf2a66ea38d5df88b5.svg" src="_images/05e7ec0d603dcc732997517b7cd9a55d8ea90a34d28f81bf2a66ea38d5df88b5.svg" /></div>
</div>
<p>You can see analytical verification of the above result in the note in the online book.</p>
<p>The risk now is that if you don’t zero the gradients for the next update iteration, you will have incorrect gradients.</p>
<blockquote>
<div><p>Always remember to zero the gradient in each iteration of the training loop!</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We can verify that the gradients are correct analytically.</p>
<p>To find the partial derivative <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial a}\)</span>, we first need to define <span class="math notranslate nohighlight">\(f\)</span> in terms of <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>Given:
$<span class="math notranslate nohighlight">\(\begin{aligned}
d &amp;= a \times b \\
e &amp;= a + b      \\
f &amp;= d \times e
\end{aligned}\)</span>$</p>
<p>Then <span class="math notranslate nohighlight">\(f\)</span> can be expanded as:
$<span class="math notranslate nohighlight">\(\begin{aligned}
f &amp;= (a \times b) \times (a + b) \\
f &amp;= a^2 \times b + a \times b^2
\end{aligned}\)</span>$</p>
<p>Next, we find the partial derivative of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(a\)</span>:
$<span class="math notranslate nohighlight">\( \frac{\partial f}{\partial a} = 2a \times b + b^2 \)</span>$</p>
<p>Finally, we plug in the given values <span class="math notranslate nohighlight">\(a = -2.0\)</span> and <span class="math notranslate nohighlight">\(b = 3.0\)</span>:
$<span class="math notranslate nohighlight">\(\begin{aligned}
\frac{\partial f}{\partial a} &amp;= 2(-2.0) \times 3.0 + 3.0^2 \\
\frac{\partial f}{\partial a} &amp;= -12.0 + 9.0                 \\
\frac{\partial f}{\partial a} &amp;= -3.0
\end{aligned}\)</span>$</p>
<p>So the partial derivative <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial a}\)</span> for the value <span class="math notranslate nohighlight">\(a = -2.0\)</span> is <span class="math notranslate nohighlight">\(-3.0\)</span>.</p>
</div>
</section>
</section>
<section id="enhancements-to-value-class">
<h2>Enhancements to <code class="docutils literal notranslate"><span class="pre">Value</span></code> Class<a class="headerlink" href="#enhancements-to-value-class" title="Permalink to this heading">#</a></h2>
<p>There are still some useful operations that <code class="docutils literal notranslate"><span class="pre">Value</span></code> doesn’t support, so to be more
complete we have the final version of the <code class="docutils literal notranslate"><span class="pre">Value</span></code> class below.</p>
<p>We added:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__radd__</span></code> for when the <code class="docutils literal notranslate"><span class="pre">Value</span></code> object is the right operand of an add</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">__rmul__</span></code> for when the <code class="docutils literal notranslate"><span class="pre">Value</span></code> object is the right operand of a product</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">__pow__</span></code> to support the ** operator</p></li>
<li><p>plus some others you can see below</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># version 9</span>
<span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">_children</span><span class="o">=</span><span class="p">(),</span> <span class="n">_op</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># default to 0, no impact on the output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="kc">None</span>  <span class="c1"># by default backward doesn&#39;t do anything</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">_children</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op</span> <span class="o">=</span> <span class="n">_op</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="s1">&#39;+&#39;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>

        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="n">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="s1">&#39;*&#39;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Adding support for ** operator, which we&#39;ll need for the </span>
<span class="sd">        squared loss function&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)),</span> <span class="s2">&quot;only supporting int/float powers for now&quot;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="sa">f</span><span class="s1">&#39;**</span><span class="si">{</span><span class="n">other</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="p">(</span><span class="n">other</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="p">(</span><span class="n">other</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="s1">&#39;ReLU&#39;</span><span class="p">)</span>
        <span class="c1"># out = Value(0 if self.data &lt; 0 else self.data, (self,), &#39;ReLU&#39;)</span>

        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="c1"># -self</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">def</span> <span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="c1"># other + self</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">+</span> <span class="n">other</span>

    <span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="c1"># self - other</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="c1"># other - self</span>
        <span class="k">return</span> <span class="n">other</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="c1"># other * self</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="n">other</span>

    <span class="k">def</span> <span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="c1"># self / other</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="n">other</span><span class="o">**-</span><span class="mi">1</span>

    <span class="k">def</span> <span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="c1"># other / self</span>
        <span class="k">return</span> <span class="n">other</span> <span class="o">*</span> <span class="bp">self</span><span class="o">**-</span><span class="mi">1</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="c1"># topological order all of the children in the graph</span>
        <span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">build_topo</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
                <span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">_prev</span><span class="p">:</span>
                    <span class="n">build_topo</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
                <span class="n">topo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">build_topo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># go one variable at a time and apply the chain rule to get its gradient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
            <span class="n">v</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a string representation of the object for display&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Value(data=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">, grad=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">)&quot;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="comparing-to-pytorch">
<h2>Comparing to PyTorch<a class="headerlink" href="#comparing-to-pytorch" title="Permalink to this heading">#</a></h2>
<p>We’re using a class implementation that resembles the PyTorch implementation, and in fact we can compare our implementation with PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>                <span class="p">;</span> <span class="n">x1</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>                <span class="p">;</span> <span class="n">x2</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>               <span class="p">;</span> <span class="n">w1</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>                <span class="p">;</span> <span class="n">w2</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">6.8813735870195432</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>  <span class="p">;</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">x1</span><span class="o">*</span><span class="n">w1</span> <span class="o">+</span> <span class="n">x2</span><span class="o">*</span><span class="n">w2</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">o</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x2.grad&#39;</span><span class="p">,</span> <span class="n">x2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;w2.grad&#39;</span><span class="p">,</span> <span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x1.grad&#39;</span><span class="p">,</span> <span class="n">x1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;w1.grad&#39;</span><span class="p">,</span> <span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.881373405456543
---
x2.grad 1.0
w2.grad 0.0
x1.grad -3.0
w1.grad 2.0
</pre></div>
</div>
</div>
</div>
<p>By default, tensors don’t store gradients and so won’t support backprop, so we explicitly set <code class="docutils literal notranslate"><span class="pre">requires_grad</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
</section>
<section id="neural-network-modules">
<h2>Neural Network Modules<a class="headerlink" href="#neural-network-modules" title="Permalink to this heading">#</a></h2>
<p>Now we’ll define some classes which help us build out a small neural network.</p>
<p><strong>Module</strong> – A base class</p>
<p><strong>Neuron</strong> – Implement a single linear or nonlinear neuron with <code class="docutils literal notranslate"><span class="pre">nin</span></code> inputs.</p>
<p><strong>Layer</strong> – Implement a layer of network consisting of <code class="docutils literal notranslate"><span class="pre">nout</span></code> neurons, each taking <code class="docutils literal notranslate"><span class="pre">nin</span></code> inputs</p>
<p><strong>MLP</strong> – A <em>Multi-Layer Perceptron</em> that implements <code class="docutils literal notranslate"><span class="pre">len(nouts)</span></code> layers of neurons.</p>
<p>Each class can calculate a forward pass and enumerate all its parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="c1"># we assume that Value class is already defined</span>

<span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define a Neural Network Module base class &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;When we run in a training loop, we&#39;ll need to zero out all the gradients</span>
<span class="sd">        since they are defined to accumulate in the backwards passes.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[]</span>

<span class="k">class</span> <span class="nc">Neuron</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define a Neuron as a subclass of Module&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nin</span><span class="p">,</span> <span class="n">nonlin</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Randomly initialize a set of weights, one for each input, and initialize the bias to zero.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">Value</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nin</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nonlin</span> <span class="o">=</span> <span class="n">nonlin</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Implement the forward pass of the neuron&quot;&quot;&quot;</span>
        <span class="n">act</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">wi</span><span class="o">*</span><span class="n">xi</span> <span class="k">for</span> <span class="n">wi</span><span class="p">,</span><span class="n">xi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">act</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlin</span> <span class="k">else</span> <span class="n">act</span>

    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;ReLU&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">nonlin</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Linear&#39;</span><span class="si">}</span><span class="s2">Neuron(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>

<span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define a Layer of Network as a subclass of Module&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nin</span><span class="p">,</span> <span class="n">nout</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize nout Neurons, each with nin inputs&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="p">[</span><span class="n">Neuron</span><span class="p">(</span><span class="n">nin</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nout</span><span class="p">)]</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass each neuron in the layer&quot;&quot;&quot;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">n</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Layer of [</span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">)</span><span class="si">}</span><span class="s2">]&quot;</span>

<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define a Multi-Layer Perceptron&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nin</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nouts</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the Multi-Layer Perceptron, by initializing each layer</span>
<span class="sd">        then initializing each neuron of each layer.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            nin: Number of inputs (int)</span>
<span class="sd">            nouts: A list of the number of neurons in each layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sz</span> <span class="o">=</span> <span class="p">[</span><span class="n">nin</span><span class="p">]</span> <span class="o">+</span> <span class="n">nouts</span>
        <span class="c1"># self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]</span>

        <span class="c1"># Create a list of layer objects for this MLP. All but the last layer</span>
        <span class="c1"># have ReLU activations. The last layer is linear.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">Layer</span><span class="p">(</span><span class="n">sz</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sz</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">nonlin</span><span class="o">=</span><span class="n">i</span><span class="o">!=</span><span class="nb">len</span><span class="p">(</span><span class="n">nouts</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nouts</span><span class="p">))]</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through the MLP&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Recursively retrieve the parameters of the MLP&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;MLP of [</span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span><span class="si">}</span><span class="s2">]&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># help(Module)</span>
<span class="c1"># Module.__doc__</span>
<span class="c1"># help(Neuron)</span>
<span class="c1"># help(Layer)</span>
<span class="c1"># help(MLP)</span>
</pre></div>
</div>
</div>
</div>
<section id="initialize-and-evaluate-a-neuron">
<h3>Initialize and Evaluate a Neuron<a class="headerlink" href="#initialize-and-evaluate-a-neuron" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2 inputs</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>

<span class="c1"># initialize neuron with 2 inputs</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">Neuron</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">nonlin</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># evaluate our neuron with our 2 inputs</span>
<span class="n">n</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=-0.06787937157518575, grad=0.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearNeuron(2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># list the 2 weights and the bias</span>
<span class="n">n</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Value(data=0.8078361682947837, grad=0.0),
 Value(data=-0.5611839027215844, grad=0.0),
 Value(data=0.0, grad=0.0)]
</pre></div>
</div>
</div>
</div>
</section>
<section id="initialize-and-evaluate-a-layer-of-neurons">
<h3>Initialize and Evaluate a Layer of Neurons<a class="headerlink" href="#initialize-and-evaluate-a-layer-of-neurons" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># same 2 inputs again</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>

<span class="c1"># Now initialize a layer of 3 neurons, each with 2 inputs</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nonlin</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Evaluate our layer of neurons with the 2 inputs</span>
<span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Value(data=-3.9257626137320827, grad=0.0),
 Value(data=3.0200247760683743, grad=0.0),
 Value(data=-1.9218817300133384, grad=0.0)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">l</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer of [LinearNeuron(2), LinearNeuron(2), LinearNeuron(2)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">l</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Value(data=-0.6915678463595973, grad=0.0),
 Value(data=-0.847542307004296, grad=0.0),
 Value(data=0.0, grad=0.0),
 Value(data=0.7868930865531618, grad=0.0),
 Value(data=0.4820795343206836, grad=0.0),
 Value(data=0.0, grad=0.0),
 Value(data=-0.7040443406866776, grad=0.0),
 Value(data=-0.17126434954666103, grad=0.0),
 Value(data=0.0, grad=0.0)]
</pre></div>
</div>
</div>
</div>
</section>
<section id="initialize-and-evaluate-an-mlp">
<h3>Initialize and Evaluate an MLP<a class="headerlink" href="#initialize-and-evaluate-an-mlp" title="Permalink to this heading">#</a></h3>
<p>We’ll instantiate an MLP like the picture below.</p>
<center>
<img src="figs/NN-figs/neural_net2.jpeg" width="75%">
</center>
<p>From <a class="reference external" href="https://cs231n.github.io/convolutional-networks/">cs231n</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=0.0, grad=0.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MLP of [Layer of [ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3)], Layer of [ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4)], Layer of [LinearNeuron(4)]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># m.parameters()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_dot</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7ee79a1c5d5d0fa05b2d6c4b50006ea86d7b7955d63f42c4ae22a7ce52e08178.svg" src="_images/7ee79a1c5d5d0fa05b2d6c4b50006ea86d7b7955d63f42c4ae22a7ce52e08178.svg" /></div>
</div>
</section>
</section>
<section id="training-loop">
<h2>Training Loop<a class="headerlink" href="#training-loop" title="Permalink to this heading">#</a></h2>
<p>So after manually iterating, we put it all together in a training loop. We can repeatedly execute the next cell to continue training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define 4 different sets of inputs</span>
<span class="n">xs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]</span>
<span class="p">]</span>

<span class="c1"># For each input set, we have a desired target value -- binary classification</span>
<span class="c1"># ys = [1.0, -1.0, -1.0, 1.0]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>

<span class="c1"># Manually seed the Random Number Generator for Reproducibility</span>
<span class="c1"># You can comment the next line out see the variability</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize an MLP with random weights</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">niters</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>

    <span class="c1"># Training Step 1: forward pass</span>
    <span class="n">ypred</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>
    
    <span class="c1"># Training Step 2: Calculate the loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">yout</span> <span class="o">-</span> <span class="n">ygt</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">ygt</span><span class="p">,</span> <span class="n">yout</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">ypred</span><span class="p">))</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Training Step 3: Zero the gradients and run the backward pass</span>
    <span class="n">m</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Training Step 4: Update parameters</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

    <span class="c1"># print(k, loss.data)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Loss: &quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iterations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss Per Iteration&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final Loss:  0.00040121534431541923
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Loss Per Iteration&#39;)
</pre></div>
</div>
<img alt="_images/4c6fbd6b744d344f50e5576a6b06932a5439ba93f50bf7cfe79be3053fa7158e.png" src="_images/4c6fbd6b744d344f50e5576a6b06932a5439ba93f50bf7cfe79be3053fa7158e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ypred</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Value(data=0.9905904618448763, grad=-0.018819076310247373),
 Value(data=0.014971101631289285, grad=0.02994220326257857),
 Value(data=-5.141668571156943e-05, grad=-0.00010283337142313886),
 Value(data=0.9905904618448763, grad=-0.018819076310247373)]
</pre></div>
</div>
</div>
</div>
</section>
<section id="build-and-train-the-equivalent-mlp-in-pytorch">
<h2>Build and Train the Equivalent MLP in PyTorch<a class="headerlink" href="#build-and-train-the-equivalent-mlp-in-pytorch" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="c1"># Manually seed the Random Number Generator for Reproducibility</span>
<span class="c1"># You can comment the next line out see the variability</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">99</span><span class="p">)</span>

<span class="c1"># Step 1: Define the MLP model</span>
<span class="k">class</span> <span class="nc">ptMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ptMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ptMLP</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Step 2: Define a loss function and an optimizer</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Step 3: Create a tiny dataset</span>
<span class="n">xs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]</span>
<span class="p">]</span>

<span class="c1"># we had to transpose ys for torch.tensor</span>
<span class="n">ys_transpose</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">],</span> 
      <span class="p">[</span><span class="mf">0.0</span><span class="p">],</span> 
      <span class="p">[</span><span class="mf">0.0</span><span class="p">],</span> 
      <span class="p">[</span><span class="mf">1.0</span><span class="p">]]</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys_transpose</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ptMLP(
  (layers): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): ReLU()
    (2): Linear(in_features=4, out_features=4, bias=True)
    (3): ReLU()
    (4): Linear(in_features=4, out_features=1, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<p>Now run the training loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 4: Write the training loop</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">niters</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>

    <span class="c1"># Training Step 1: Forward pass</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Training Step 2: Calculate the loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="c1"># Training Step 3: Zero the gradient and run backward pass</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Training Step 4: Update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="c1"># print(f&#39;Epoch {epoch+1}, Loss: {loss.item()}&#39;)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Final Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iterations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss Per Iteration&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final Loss: 0.06534020602703094
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Loss Per Iteration&#39;)
</pre></div>
</div>
<img alt="_images/11b524295f07d22a7127f913782af565cc44dd187978d2d4c85ec637bd0804f4.png" src="_images/11b524295f07d22a7127f913782af565cc44dd187978d2d4c85ec637bd0804f4.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1.1306],
        [0.0364],
        [0.0176],
        [0.7840]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="to-dig-a-little-deeper">
<h2>To Dig a Little Deeper<a class="headerlink" href="#to-dig-a-little-deeper" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/intro.html">PyTorch Quick Start Tutorial</a></p>
<p><a class="reference external" href="https://playground.tensorflow.org/#activation=relu&amp;batchSize=30&amp;dataset=gauss&amp;regDataset=reg-plane&amp;learningRate=0.01&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,4&amp;seed=0.75152&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">TensorFlow Playground</a></p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<p>So today we…</p>
<ul class="simple">
<li><p>got a glimpse of the wide applications of neural networks</p></li>
<li><p>revisited loss functions</p></li>
<li><p>developed the notion of gradient descent first by intuition, then in the univariate case, then the multivariate case</p></li>
<li><p>defined artificial neurons</p></li>
<li><p>implemented a computation graph and visualization</p></li>
<li><p>implemented the chain rule as backpropagation on the computation graph</p></li>
<li><p>defined Neuron, Layer and MLP modules which completes are homegrown Neural Network Framework</p></li>
<li><p>then trained a small MLP on a tiny dataset</p></li>
<li><p>finally implemented the same in PyTorch</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="23-NN-I-Gradient-Descent.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Neural Networks I – Gradient Descent</p>
      </div>
    </a>
    <a class="right-next"
       href="25-NN-III-CNNs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">NN III – Stochastic Gradient Descent, Batches and Convolutional Neural Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neuron-and-neural-networks-recap">Neuron and Neural Networks (Recap)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-neuron">Artificial Neuron</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relating-back-to-earlier-lectures">Relating Back to Earlier Lectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relating-back-to-another-earlier-lecture">Relating Back to <em>Another</em> Earlier Lecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-layer-perceptron-mlp-or-fully-connected-network-fcn">Multi-Layer Perceptron (MLP) or Fully Connected Network (FCN)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-graph">Computation Graph</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-value-class">Building the <code class="docutils literal notranslate"><span class="pre">Value</span></code> Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-addition">Implementing Addition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-more-operations">Implementing More Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#child-nodes">Child Nodes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#child-operations">Child Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-compute-graph">The Compute Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recap">Recap</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-gradients">Calculating Gradients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-gradient-calculation">Manual Gradient Calculation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#propagating-back">Propagating Back</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question">Question</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule">The Chain Rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#propagating-back-again">Propagating Back Again</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Recap</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-step-in-optimization">A Step in Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-single-neuron">A Single Neuron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coding-backpropagation">Coding Backpropagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accumulating-the-gradients">Accumulating the Gradients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#enhancements-to-value-class">Enhancements to <code class="docutils literal notranslate"><span class="pre">Value</span></code> Class</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-to-pytorch">Comparing to PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-modules">Neural Network Modules</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-and-evaluate-a-neuron">Initialize and Evaluate a Neuron</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-and-evaluate-a-layer-of-neurons">Initialize and Evaluate a Layer of Neurons</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-and-evaluate-an-mlp">Initialize and Evaluate an MLP</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop">Training Loop</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-and-train-the-equivalent-mlp-in-pytorch">Build and Train the Equivalent MLP in PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#to-dig-a-little-deeper">To Dig a Little Deeper</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mark Crovella and Tom Gardos
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>